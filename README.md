# chengxuchu
eee
情境是这样的：任务是进行机组一天的出力决策，使一天总24个小时的总花费(huafei)最小。研究案例中的电力系统包含发电机、光伏、蓄电池，上级电网及用户。
状态量是用户负荷需求(load)、光伏发电(pv)和电池的荷电状态SOC共3个量。其中，每个时刻的load 和 pv 从数据集中获取，soc已知第一个时刻的初始值。
动作量包括发电机发电（pb_output）和蓄电池充/放电（pc_output），还包括从上级电网购/售电量（pd_output），因此动作量包括这3个。

进行了1000次训练，每次训练用的一天(24小时，1个小时算作1步，一个episode共24步) 的状态数据（包括load和pv）和另外一天的数据是不同的，
，但是初始的soc都设成了0.6。。但是这1000天中每一天的电价和另外一天的电价是相同的，如今天9点和明天9点买电的电价都是0.49元，
卖电的电价都是0.38元；今天12点和明天12点买电的电价都是0.83元，卖电的电价都是0.65元。
蓄电池在买电电价高的时候向系统放电，在买电电价低的时候进行充电（为了省钱）。


该系统的状态空间中的前两个变量虽然不会随行为(动作action)改变，但是蓄电池的状态soc是会随行为改变的，它与蓄电池的出力fb_output存在一定的关系
（本文是给出了每一天0点soc的初值）；相当于虽然系统状态变量中的一部分是从环境中直接观测的，但还有一部分是与环境交互反馈得到的。

Github中的程序DQN不用看，因为之前打算将连续动作离散化，但是没必要，也就没有删除。

蓄电池的soc被限制在[0.2,0.8]内，如果超出此范围，则会产生惩罚成本，因为蓄电池的深度充放电会损害电池的使用寿命。

reward由两部分组成：第一大部分是成本(reward_chengben)，包括发电机发电的成本和从主电网购/售电的成本；
第二大部分是蓄电池的处罚金额(reward_penalty)，将蓄电池的soc设置在[0.2,0.8]的范围内，如果超过该范围，将产生处罚成本。

其中， SOC的更新语句为：
self.current_soc = self.current_soc - pc_output / 2000   ###假设蓄电池的容量为200KW

我先在shuju.csv中存储了24000+行的数据（状态量中load和pv）。相当于有两个大循环，外边的大循环是训练学习的次数(MAX_EPISODES = 1000)，里边的循环是每次训练学习要走的步数（从1点到24点，MAX_EP_STEPS = 23）。训练1000次，第一次要依次读第1到24个；episode为第2次训练时再读取第25到48个；……

我的思路是先让强化学习学习1000/10000次(MAX_EPISODES = 1000)，当模型收敛后，再采用训练好的模型对发电机、蓄电池和上级电网进行出力规划，使出力的成本huafei最小。
 if env.fdpower(action) >= 0:   ###env.fdpower(action) >= 0代表从上级电网买电，买电贵点儿
            huafei = huafei + env.fbpower(action) * env.fbpower(action) * 0.32 + env.fdpower(action) * dianjiabuy  ###定义的总的花费的成本
else            ##代表把多余的用户用不了的电（光伏发电多产生的，蓄电池也消化不了的）卖给上级电网，，卖电便宜点儿
            huafei = huafei + env.fbpower(action) * env.fbpower(action) * 0.32 + env.fdpower(action) * dianjiabuy  ###定义的总的花费的成本
